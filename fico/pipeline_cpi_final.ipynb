{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at allenai/longformer-large-4096 were not used when initializing LongformerModel: ['lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing LongformerModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing LongformerModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "c:\\Users\\thgcn\\OneDrive\\Academico\\PO-245\\Projeto\\PO245\\po_245_2023_T4\\.venv\\Lib\\site-packages\\transformers\\models\\t5\\tokenization_t5.py:163: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Este módulo contém funções e bibliotecas relacionadas ao pipeline de captura,\n",
    "pré-processamento e indexação de relatórios.\n",
    "\"\"\"  # noqa: D205\n",
    "\n",
    "import io\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import subprocess\n",
    "import zipfile\n",
    "from pathlib import Path\n",
    "\n",
    "import PyPDF2\n",
    "import requests\n",
    "import spacy\n",
    "import torch\n",
    "import torch.nn.functional as fun\n",
    "import unidecode\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from transformers import (\n",
    "    BertForQuestionAnswering,\n",
    "    LongformerModel,\n",
    "    LongformerTokenizer,\n",
    "    T5ForConditionalGeneration,\n",
    "    T5Tokenizer,\n",
    ")\n",
    "\n",
    "model_for_question = BertForQuestionAnswering.from_pretrained(\n",
    "    \"bert-large-uncased-whole-word-masking-finetuned-squad\",\n",
    ")\n",
    "tokenizer = LongformerTokenizer.from_pretrained(\"allenai/longformer-large-4096\")\n",
    "model = LongformerModel.from_pretrained(\"allenai/longformer-large-4096\")\n",
    "model_sum = T5ForConditionalGeneration.from_pretrained(\"t5-base\")\n",
    "tokenizer_sum = T5Tokenizer.from_pretrained(\"t5-base\")\n",
    "\n",
    "def verificar_diretorio():\n",
    "    \"\"\"Verifica se o diretório informado pelo usuário é válido.\n",
    "\n",
    "    Returns:\n",
    "        str: O caminho absoluto do diretório válido.\n",
    "\n",
    "    Example:\n",
    "        >>> verificar_diretorio()\n",
    "        Digite o caminho do diretório: /caminho/do/diretorio\n",
    "        Diretório raiz para armazenamento dos documentos: /caminho/do/diretorio\n",
    "        '/caminho/do/diretorio'\n",
    "\n",
    "    \"\"\"\n",
    "    root = Path(input(\"Digite o caminho do diretório: \"))\n",
    "    try:\n",
    "        if root.is_dir():\n",
    "            print(\"Diretório raiz para armazenamento dos documentos: \" + str(root))\n",
    "            return root\n",
    "    except FileNotFoundError:\n",
    "        print(\"O diretório inserido não existe ou não é válido. \\\n",
    "            Certifique-se que o nome está correto\")\n",
    "\n",
    "def busca_ipe(ano):\n",
    "    \"\"\"Verifica se o diretório informado pelo usuário é válido.\n",
    "\n",
    "    Returns:\n",
    "        str: O caminho absoluto do diretório válido.\n",
    "\n",
    "    Example:\n",
    "        >>> verificar_diretorio()\n",
    "        Digite o caminho do diretório: /caminho/do/diretorio\n",
    "        Diretório raiz para armazenamento dos documentos: /caminho/do/diretorio\n",
    "        '/caminho/do/diretorio'\n",
    "\n",
    "    \"\"\"\n",
    "    url = \"https://dados.cvm.gov.br/dados/CIA_ABERTA/DOC/IPE/DADOS/\"\n",
    "    url += \"ipe_cia_aberta_%d.zip\" % ano\n",
    "    file = \"ipe_cia_aberta_%d.zip\" % ano\n",
    "    r = requests.get(url)\n",
    "    zf = zipfile.ZipFile(io.BytesIO(r.content))\n",
    "    file = zf.namelist()\n",
    "    zf = zf.open(file[0])\n",
    "    lines = zf.readlines()\n",
    "    lines = [i.strip().decode(\"ISO-8859-1\") for i in lines]\n",
    "    lines = [i.split(\";\") for i in lines]\n",
    "    return lines\n",
    "\n",
    "def search(lista, valor):\n",
    "    \"\"\"Retorna uma lista de elementos da lista que contêm o valor especificado.\n",
    "\n",
    "    Args:\n",
    "        lista (list): A lista de elementos para realizar a busca.\n",
    "        valor: O valor a ser procurado nos elementos da lista.\n",
    "\n",
    "    Returns:\n",
    "        list: Uma lista dos elementos que contêm o valor especificado.\n",
    "\n",
    "    Example:\n",
    "        >>> search(['apple', 'banana', 'orange'], 'an')\n",
    "        ['banana', 'orange']\n",
    "\n",
    "    \"\"\"\n",
    "    return [(lista[lista.index(x)]) for x in lista if valor in x]\n",
    "\n",
    "\n",
    "def baixar_arquivo(url, endereco):\n",
    "    \"\"\"Faz o download de um arquivo a partir de uma URL e salva no caminho especificado.\n",
    "\n",
    "    Args:\n",
    "        url (str): A URL do arquivo a ser baixado.\n",
    "        endereco (str): O caminho completo de destino para salvar o arquivo.\n",
    "\n",
    "    Raises:\n",
    "        Exception: Se ocorrer um erro ao fazer o download do arquivo.\n",
    "\n",
    "    Example:\n",
    "        >>> baixar_arquivo('https://example.com/file.pdf', '/path/to/save/file.pdf')\n",
    "        Download finalizado. Arquivo salvo em: /path/to/save/file.pdf\n",
    "\n",
    "    \"\"\"\n",
    "    resposta = requests.get(url, allow_redirects=True, verify=False, stream=True)\n",
    "    if resposta.status_code == requests.codes.OK:\n",
    "        with endereco.open(\"wb\") as novo_arquivo:\n",
    "            novo_arquivo.write(resposta.content)\n",
    "        print(f\"Download finalizado. Arquivo salvo em: {endereco}\")\n",
    "    else:\n",
    "        resposta.raise_for_status()\n",
    "\n",
    "\n",
    "def transform_string(text):\n",
    "    \"\"\"Remove acentos, substitui espaços por underline, converte para letras minúsculas.\n",
    "\n",
    "    Args:\n",
    "        text (str): A string a ser transformada.\n",
    "\n",
    "    Returns:\n",
    "        str: A string transformada.\n",
    "\n",
    "    Example:\n",
    "        >>> transform_string(\"Olá, Mundo!\")\n",
    "        'ola_mundo'\n",
    "\n",
    "    \"\"\"\n",
    "    text = unidecode.unidecode(text)\n",
    "    text = text.replace(\" \", \"_\")\n",
    "    text = text.replace(\".\", \"\")\n",
    "    text = text.lower()\n",
    "    return text\n",
    "\n",
    "\n",
    "def download_def(empresa, year, root):  # noqa: C901, PLR0915\n",
    "    \"\"\"Realiza download de arquivos específicos com base na empresa e no ano fornecidos.\n",
    "\n",
    "    Args:\n",
    "        empresa (str): Nome da empresa para qual deseja fazer o download dos arquivos.\n",
    "        year (int): O ano para o qual deseja-se fazer o download dos arquivos.\n",
    "        root(str):  Caminho onde serao armazenados os documentos\n",
    "\n",
    "    Returns:\n",
    "        list: Uma lista de dicionários contendo os metadados dos arquivos baixados.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: Se houver duplicidade de empresas encontradas.\n",
    "\n",
    "    Example:\n",
    "        >>> download_def(\"Empresa XYZ\", 2023)\n",
    "        Empresa encontrada: Empresa XYZ | codigo cvm: 12345\n",
    "        Ano: 2023\n",
    "        nome do arquivo: 12345_empresa_xyz\n",
    "        caminho dos arquivos: /caminho/do/arquivo/12345_empresa_xyz\n",
    "        ...\n",
    "\n",
    "    \"\"\"\n",
    "    lines = busca_ipe(year)\n",
    "    defi = search(lines, \"Dados Econômico-Financeiros\")\n",
    "    data = []\n",
    "    empresa_name = \"\"\n",
    "    num_cvm = \"\"\n",
    "    for a in range(len(defi)):\n",
    "        if empresa.upper() in defi[a][1]: \n",
    "            empresa_name = defi[a][1]\n",
    "            num_cvm = defi[a][2]\n",
    "            print(\"Empresa encontrada: \" + empresa_name + \" | codigo cvm: \" + num_cvm)\n",
    "            print(\"Ano: \" + str(year))\n",
    "            sufix = transform_string(defi[a][2] + \"_\" + defi[a][1])\n",
    "            company = root / sufix\n",
    "            category = transform_string(defi[a][5])\n",
    "            print(\"nome do arquivo: \" + sufix)\n",
    "            print(\"caminho dos arquivos: \" + str(company))\n",
    "            if not company.is_dir():\n",
    "                print(\"diretorio não existe e será criado\")\n",
    "                company.mkdir(parents=True)\n",
    "            diryear = company / str(year)\n",
    "            if not diryear.is_dir():\n",
    "                print(\"diretorio não existe e será criado\")\n",
    "                diryear.mkdir(parents=True)\n",
    "            dircategory = diryear / category\n",
    "            if not dircategory.is_dir():\n",
    "                print(\"diretorio não existe e será criado\")\n",
    "                dircategory.mkdir(parents=True)\n",
    "            url = defi[a][12]\n",
    "            nome = (\n",
    "                transform_string(defi[a][2])\n",
    "                + \"_\"\n",
    "                + defi[a][1]\n",
    "                + \"_\"\n",
    "                + defi[a][7][1:50]\n",
    "                + \"_\"\n",
    "                + defi[a][8]\n",
    "            )[1:100]\n",
    "\n",
    "            baixar_arquivo(url, dircategory / f\"{nome}.pdf\")\n",
    "\n",
    "            title = transform_string(defi[a][6][0:50])\n",
    "            if not title:\n",
    "                if not defi[a][7]:\n",
    "                    title = defi[a][7]\n",
    "                elif not defi[a][5]:\n",
    "                    title = defi[a][5]\n",
    "                elif not defi[a][4]:\n",
    "                    title = defi[a][4]\n",
    "                else:\n",
    "                    title = defi[a][1]\n",
    "            company_name = transform_string(defi[a][1])\n",
    "            cod_cvm = defi[a][2]\n",
    "            date = defi[a][3]\n",
    "            content = defi[a][7]\n",
    "            if not content:\n",
    "                content = defi[a][6]\n",
    "            keyword = defi[a][4]\n",
    "            file_path = dircategory / f\"{nome}.pdf\"\n",
    "            size = file_path.stat().st_size\n",
    "            file = str(file_path)\n",
    "            data.append({\n",
    "                \"name\": title,\n",
    "                \"type\": \"string\",\n",
    "                \"company_name\": company_name,\n",
    "                \"cod_cvm\": cod_cvm,\n",
    "                \"content\": content,\n",
    "                \"year\": year,\n",
    "                \"date\": date,\n",
    "                \"keywords\": keyword,\n",
    "                \"size\": size,\n",
    "                \"tokens\": \"\",\n",
    "                \"tensor\": \"\",\n",
    "                \"file\": file,\n",
    "            })\n",
    "    return data\n",
    "\n",
    "\n",
    "def convert_pdf(doc):\n",
    "    \"\"\"Converte um arquivo PDF em texto.\n",
    "\n",
    "    Args:\n",
    "        doc (str): O caminho do arquivo PDF a ser convertido.\n",
    "\n",
    "    Returns:\n",
    "        str: O texto extraído do arquivo PDF.\n",
    "\n",
    "    Example:\n",
    "        text = convert_pdf(\"path/to/file.pdf\")\n",
    "        print(text)\n",
    "        # Output: Text extracted from the PDF file.\n",
    "\n",
    "    \"\"\"\n",
    "    with Path(doc).open(\"rb\") as f:\n",
    "        # Use a biblioteca PyPDF2 para ler o arquivo\n",
    "        reader = PyPDF2.PdfReader(f)\n",
    "        # Obtenha o número de páginas no arquivo\n",
    "        num_pages = len(reader.pages)\n",
    "        # String vazia para armazenar o texto extraído\n",
    "        text = \"\"\n",
    "        # Itere pelas páginas e extraia o texto\n",
    "        for page in range(num_pages):\n",
    "            # Obtenha o objeto da página\n",
    "            # page_obj = reader.getPage(page)\n",
    "            page_obj = reader.pages[page]\n",
    "            # Extraia o texto da página\n",
    "            # page_text = page_obj.extractText()\n",
    "            page_text = page_obj.extract_text()\n",
    "            # Adicione o texto extraído à string de texto\n",
    "            text += page_text\n",
    "    return text\n",
    "\n",
    "def remover_stopwords(tokens):\n",
    "    \"\"\"Remove as stopwords de uma lista de tokens.\n",
    "\n",
    "    Args:\n",
    "        tokens (list): A lista de tokens.\n",
    "\n",
    "    Returns:\n",
    "        list: A lista de tokens sem as stopwords.\n",
    "\n",
    "    Example:\n",
    "        token_list = ['This', 'is', 'a', 'sample', 'sentence']\n",
    "        tokens_without_stopwords = remover_stopwords(token_list)\n",
    "        print(tokens_without_stopwords)\n",
    "        # Output: ['This', 'sample', 'sentence']\n",
    "\n",
    "    \"\"\"\n",
    "    # Carrega o modelo do SpaCy para o idioma português\n",
    "    nlp = spacy.load(\"pt_core_news_lg\")\n",
    "\n",
    "    # Cria uma lista para armazenar os tokens sem stopwords\n",
    "    tokens_sem_stopwords = []\n",
    "\n",
    "    # Percorre cada token na lista\n",
    "    for token in tokens:\n",
    "        # Verifica se o token não é uma stopword\n",
    "        if not nlp.vocab[token].is_stop:\n",
    "            tokens_sem_stopwords.append(token)\n",
    "    return tokens_sem_stopwords\n",
    "\n",
    "\n",
    "def normalize_text(text):\n",
    "    \"\"\"Normaliza o texto, convertendo-o para minúsculas e removendo caracteres especiais\n",
    "    e acentuação.\n",
    "\n",
    "    Args:\n",
    "        text (str): O texto a ser normalizado.\n",
    "\n",
    "    Returns:\n",
    "        str: O texto normalizado.\n",
    "\n",
    "    Example:\n",
    "        text = \"Hello, World! This is an example text.\"\n",
    "        normalized_text = normalize_text(text)\n",
    "        print(normalized_text)\n",
    "        # Output: hello world this is an example text\n",
    "\n",
    "    \"\"\"  # noqa: D205\n",
    "    # Converte o texto para minúsculas\n",
    "    normalized_text = text.lower()\n",
    "    # Remove caracteres especiais e acentuação\n",
    "    normalized_text = re.sub(r\"[^\\w\\s]\", \"\", normalized_text)\n",
    "    # Retorna o texto limpo\n",
    "    return normalized_text\n",
    "\n",
    "def tokengen(text):\n",
    "    \"\"\"Gera uma lista de tokens a partir de um texto.\n",
    "\n",
    "    Args:\n",
    "        text (str): O texto a ser tokenizado.\n",
    "\n",
    "    Returns:\n",
    "        list: Uma lista de tokens.\n",
    "\n",
    "    Example:\n",
    "        text = \"Hello, World! This is an example text.\"\n",
    "        tokens = tokengen(text)\n",
    "        print(tokens)\n",
    "        # Output: ['[CLS]', 'hello', ',', 'world', '!', 'this', 'is', 'an', 'example',\n",
    "        # 'text', '.', '[SEP]']\n",
    "\n",
    "    \"\"\"\n",
    "    tokens = tokenizer.encode(text, add_special_tokens=True)\n",
    "    return tokenizer.convert_ids_to_tokens(tokens)\n",
    "\n",
    "def vector_one(text):\n",
    "    \"\"\"Gera um vetor de representação para um texto.\n",
    "\n",
    "    Args:\n",
    "        text (str): O texto a ser vetorizado.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: O vetor de representação do texto.\n",
    "\n",
    "    Example:\n",
    "        text = \"This is a sample text.\"\n",
    "        text_vector = vector_one(text)\n",
    "        print(text_vector)\n",
    "        # Output: torch.Tensor representing the vector for the text.\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    tokens = tokenizer.encode(text, add_special_tokens=True)\n",
    "    input_ids = torch.tensor(tokens).unsqueeze(0)\n",
    "\n",
    "    max_length = 4096\n",
    "    vectors = []\n",
    "    # Divide the tokenized text into parts\n",
    "    input_parts = [\n",
    "    input_ids[:, i:i+max_length]\n",
    "    for i in range(0, input_ids.shape[1], max_length)\n",
    "    ]\n",
    "    # Process each part of the tokenized text\n",
    "    for part in input_parts:\n",
    "        # Generate the embeddings vectors\n",
    "        with torch.no_grad():\n",
    "            outputs = model(part)\n",
    "            last_hidden_states = outputs.last_hidden_state\n",
    "\n",
    "        # Calculate the average vector\n",
    "        average_vector = torch.mean(last_hidden_states, dim=1)\n",
    "\n",
    "        # Add the average vector to the list\n",
    "        vectors.append(average_vector)\n",
    "\n",
    "    # Concatenate the generated vectors\n",
    "    return torch.cat(vectors, dim=1)\n",
    "\n",
    "def similarity_vector(a, b):\n",
    "    \"\"\"Calcula a similaridade entre dois vetores.\n",
    "\n",
    "    Args:\n",
    "        a (torch.Tensor): O primeiro vetor.\n",
    "        b (torch.Tensor): O segundo vetor.\n",
    "\n",
    "    Returns:\n",
    "        float: O valor da similaridade entre os vetores.\n",
    "\n",
    "    Example:\n",
    "        vector1 = torch.tensor([0.1, 0.2, 0.3])\n",
    "        vector2 = torch.tensor([0.4, 0.5, 0.6])\n",
    "        similarity = similarity_vector(vector1, vector2)\n",
    "        print(similarity)\n",
    "        # Output: Similarity score between the vectors.\n",
    "\n",
    "    \"\"\"\n",
    "    size_a = a.size(0)\n",
    "    size_b = b.size(0)\n",
    "    if size_a == size_b:\n",
    "        # similarity_score = cosine_similarity(a, b)\n",
    "        similarity_score = cosine_similarity(a.float(), b.float())\n",
    "        return similarity_score.item()\n",
    "    if size_a > size_b:\n",
    "        diff = size_a - size_b\n",
    "        padded = fun.pad(b, pad=(0, diff), mode=\"constant\", value=0)\n",
    "        similarity_score = cosine_similarity(a.float(), padded.float())\n",
    "        return similarity_score.item()\n",
    "    if size_a < size_b:\n",
    "        diff = size_b - size_a\n",
    "        padded = fun.pad(a, pad=(0, diff), mode=\"constant\", value=0)\n",
    "        similarity_score = cosine_similarity(b.float(), padded.float())\n",
    "        return similarity_score.item()\n",
    "    return None\n",
    "\n",
    "def question(text, question):\n",
    "    \"\"\"Calcula a resposta para uma pergunta com base em um texto usando\n",
    "    um modelo de similaridade.\n",
    "\n",
    "    Args:\n",
    "        text (str): O texto em que a pergunta será feita.\n",
    "        question (str): A pergunta a ser respondida.\n",
    "\n",
    "    Returns:\n",
    "        str: A resposta para a pergunta.\n",
    "\n",
    "    Example:\n",
    "        text = \"Lorem ipsum dolor sit amet, consectetur adipiscing elit.\"\n",
    "        question = \"Qual é o significado de Lorem ipsum?\"\n",
    "        answer = question(text, question)\n",
    "        print(answer)\n",
    "        # Output: Resposta para a pergunta..\n",
    "\n",
    "    \"\"\"  # noqa: D205\n",
    "    question = question\n",
    "    max_answer_length = 512\n",
    "    max_length = 512\n",
    "    input_parts = []\n",
    "    input_parts = [text[i:i+max_length] for i in range(0, len(text), max_length)]\n",
    "    # Lista para armazenar as respostas\n",
    "    answers = []\n",
    "    # Processa cada parte do texto\n",
    "    for part in input_parts:\n",
    "        # Tokeniza a parte do texto\n",
    "        # Tokeniza a pergunta e o texto menor\n",
    "        encoding = tokenizer.encode_plus(question, part, return_tensors=\"pt\",\n",
    "                                         max_length=512, truncation=True)\n",
    "        input_ids = encoding[\"input_ids\"]\n",
    "        attention_mask = encoding[\"attention_mask\"]\n",
    "        # Obtém as respostas do modelo pré-treinado\n",
    "        with torch.no_grad():\n",
    "            outputs = model_for_question(\n",
    "                input_ids=input_ids, attention_mask=attention_mask)\n",
    "            start_scores = outputs.start_logits\n",
    "            end_scores = outputs.end_logits\n",
    "        # Obtém a resposta com maior probabilidade\n",
    "        answer_start = torch.argmax(start_scores)\n",
    "        answer_end = torch.argmax(end_scores)\n",
    "        answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(\n",
    "            input_ids[0][answer_start.item():answer_end.item()+1]))\n",
    "        # Adiciona a resposta à lista\n",
    "        answers.append(answer)\n",
    "    # Combina as respostas\n",
    "    combined_answer = ' '.join(answers)\n",
    "    # Limita o tamanho da resposta final\n",
    "    combined_answer = combined_answer[:max_answer_length]\n",
    "        # Formatação da resposta final\n",
    "    formatted_answer = \"RESPOSTA:\\n\\n\"\n",
    "    formatted_answer += combined_answer.replace(\".\", \".\\n\\n\")\n",
    "    # Apresenta a resposta final\n",
    "    return formatted_answer\n",
    "\n",
    "def summarization(text):\n",
    "    \"\"\"Gera um resumo do texto fornecido.\n",
    "\n",
    "    Args:\n",
    "        text (str): O texto a ser resumido.\n",
    "\n",
    "    Returns:\n",
    "        str: O resumo gerado do texto.\n",
    "\n",
    "    Example:\n",
    "        text = \"This is a sample text. It contains multiple sentences.\n",
    "        The goal is to generate a summary.\"\n",
    "        summary = summarization(text)\n",
    "        print(summary)\n",
    "        # Output: A summary of the text.\n",
    "\n",
    "    \"\"\"\n",
    "    num_sentences = 5\n",
    "    # Tokenizar o texto em partes\n",
    "    input_parts = text.split(\". \")\n",
    "    summaries = []\n",
    "    for part in input_parts:\n",
    "        length = len(part)\n",
    "        m_length = int(length * 0.4)\n",
    "        # Tokenizar a parte do texto\n",
    "        inputs = tokenizer_sum.encode(\"summarize: \" + part, return_tensors=\"pt\")\n",
    "        # Gerar o resumo da parte do texto usando o modelo T5\n",
    "        outputs = model_sum.generate(\n",
    "            inputs, max_length=m_length, num_beams=4, early_stopping=True)\n",
    "        summary = tokenizer_sum.decode(outputs[0], skip_special_tokens=True)\n",
    "        # Adicionar o resumo à lista de sumários\n",
    "        summaries.append(summary)\n",
    "\n",
    "    # Concatenar os sumários em um único texto\n",
    "    concatenated_summary = \" \".join(summaries)\n",
    "\n",
    "    # Extrair as N sentenças mais importantes\n",
    "    sentences = concatenated_summary.split(\". \")\n",
    "    top_sentences = sorted(\n",
    "        sentences, key=lambda x: len(x), reverse=True)[:num_sentences]\n",
    "\n",
    "    return \". \".join(top_sentences)\n",
    "\n",
    "def start_solr_service(solr_bin_path):\n",
    "    \"\"\"Inicia o serviço do Solr.\n",
    "\n",
    "    Args:\n",
    "        solr_bin_path (str): O caminho para o diretório bin do Solr.\n",
    "\n",
    "    Returns:\n",
    "        int: O código de retorno da execução do comando.\n",
    "    \"\"\"\n",
    "    # Comando para iniciar o serviço do Solr\n",
    "    command = f\"{solr_bin_path}solr start -port 8983\"\n",
    "\n",
    "    # Executa o comando no terminal\n",
    "    result = subprocess.run(command, shell=True)\n",
    "\n",
    "    # Retorna o código de retorno da execução do comando\n",
    "    return result.returncode\n",
    "\n",
    "\n",
    "def check_collection_exists(collection_name):\n",
    "    \"\"\"Verifica se uma coleção existe no servidor Solr.\n",
    "\n",
    "    Args:\n",
    "        collection_name (str): O nome da coleção a ser verificada.\n",
    "\n",
    "    Returns:\n",
    "        bool: True se a coleção existe, False caso contrário.\n",
    "\n",
    "    Example:\n",
    "        >>> check_collection_exists(\"my_collection\")\n",
    "        True\n",
    "\n",
    "    \"\"\"\n",
    "    url = f\"http://localhost:8983/solr/{collection_name}/admin/ping\"\n",
    "\n",
    "    response = requests.get(url)\n",
    "    status_ok = 200\n",
    "    status_not_found = 404\n",
    "    if response.status_code == status_ok:\n",
    "        return True\n",
    "    elif response.status_code == status_not_found:  # noqa: RET505\n",
    "        return False\n",
    "    else:\n",
    "        print(\"Falha ao verificar a existência da coleção. Status:\",\n",
    "              response.status_code)\n",
    "        return False\n",
    "\n",
    "def create_solr_collection(collection_name):\n",
    "    \"\"\"Cria uma coleção no servidor Solr e atualiza o esquema.\n",
    "\n",
    "    Args:\n",
    "        collection_name (str): O nome da coleção a ser criada.\n",
    "\n",
    "    Example:\n",
    "        >>> create_solr_collection(\"my_collection\")\n",
    "        Esquema atualizado com sucesso.\n",
    "\n",
    "    \"\"\"\n",
    "    # Caminho para o diretório bin do Solr\n",
    "    solr = \"C:\\\\Users\\\\thgcn\\\\OneDrive\\\\Academico\\\\PO-245\\\\Projeto\\\\solr-9.2.1\\\\bin\\\\\"\n",
    "\n",
    "    # Mude para o diretório bin do Solr\n",
    "    os.chdir(solr)\n",
    "\n",
    "    # Comando para criar a coleção no modo standalone\n",
    "    create_collection_command = f\"solr.cmd create -c {collection_name} -s 2 -rf 2\"\n",
    "\n",
    "    # Execute o comando no terminal\n",
    "    subprocess.run(create_collection_command, shell=True)\n",
    "\n",
    "    # URL do endpoint Solr\n",
    "    url = f\"http://localhost:8983/solr/{collection_name}/schema\"\n",
    "\n",
    "    # Cabeçalhos da solicitação POST\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\",\n",
    "    }\n",
    "\n",
    "    # Corpo da solicitação POST\n",
    "    data = {\n",
    "        \"add-field\": [\n",
    "            {\"name\": \"name\", \"type\": \"text_general\", \"multiValued\": False},\n",
    "            {\"name\": \"company_name\", \"type\": \"text_general\", \"multiValued\": False},\n",
    "            {\"name\": \"cod_cvm\", \"type\": \"text_general\", \"multiValued\": False},\n",
    "            {\"name\": \"content\", \"type\": \"text_general\", \"multiValued\": True},\n",
    "            {\"name\": \"year\", \"type\": \"pint\"},\n",
    "            {\"name\": \"date\", \"type\": \"pdate\"},\n",
    "            {\"name\": \"keywords\", \"type\": \"text_general\", \"multiValued\": True},\n",
    "            {\"name\": \"size\", \"type\": \"pint\"},\n",
    "            {\"name\": \"tokens\", \"type\": \"text_general\", \"multiValued\": False},\n",
    "            {\"name\": \"tensor\", \"type\": \"text_general\", \"multiValued\": False},\n",
    "            {\"name\": \"file\", \"type\": \"text_general\", \"multiValued\": False}\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    # Enviar solicitação POST para atualizar o esquema no Solr\n",
    "    response = requests.post(url, json=data, headers=headers)\n",
    "    status_ok = 200\n",
    "\n",
    "    if response.status_code == status_ok:\n",
    "        print(\"Esquema atualizado com sucesso.\")\n",
    "    else:\n",
    "        print(\"Falha ao atualizar o esquema. Status:\", response.status_code)\n",
    "\n",
    "def update_schema(data, collection_name):\n",
    "    \"\"\"Atualiza o esquema da coleção Solr com os campos fornecidos.\n",
    "\n",
    "    Args:\n",
    "        data (dict): Dicionário contendo os campos e configurações\n",
    "        a serem adicionados ao esquema.\n",
    "        collection_name (str): O nome da coleção a ser atualizada.\n",
    "\n",
    "    Example:\n",
    "        data = {\n",
    "            \"add-field\": [\n",
    "                {\"name\": \"title\", \"type\": \"text_general\", \"multiValued\": False},\n",
    "                {\"name\": \"content\", \"type\": \"text_general\", \"multiValued\": True},\n",
    "                {\"name\": \"author\", \"type\": \"text_general\", \"multiValued\": False}\n",
    "            ]\n",
    "        }\n",
    "        update_schema(data, \"my_collection\")\n",
    "\n",
    "    \"\"\"\n",
    "    # URL do endpoint Solr\n",
    "    url = f\"http://localhost:8983/solr/{collection_name}/schema\"\n",
    "\n",
    "    # Cabeçalhos da solicitação POST\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\"}\n",
    "\n",
    "    # Converter tensores em listas\n",
    "    data = json.loads(json.dumps(data, default=lambda x: x.tolist()))\n",
    "\n",
    "    # Enviar solicitação POST para atualizar o esquema no Solr\n",
    "    response = requests.post(url, json=data, headers=headers)\n",
    "\n",
    "    upd = f\"http://localhost:8983/solr/{collection_name}/config\"\n",
    "    status_ok = 200\n",
    "    if response.status_code == status_ok:\n",
    "        print(\"1/2 Esquema atualizado com sucesso.\")\n",
    "        data_up = {\n",
    "        \"set-property\": {\n",
    "            \"updateHandler.autoCommit.maxTime\": 15000,\n",
    "            },\n",
    "        }\n",
    "        response = requests.post(upd, headers=headers, json=data_up)\n",
    "        if response.status_code == status_ok:\n",
    "            print(\"2/2 Commit realizado com sucesso.\")\n",
    "        else:\n",
    "            print(\"Falha no commit. Status:\", response.status_code)\n",
    "            print(response.content.decode())\n",
    "    else:\n",
    "        print(\"Falha ao atualizar o esquema. Status:\", response.status_code)\n",
    "        print(response.content.decode())\n",
    "\n",
    "def add_document_to_solr(collection_name, document):\n",
    "    \"\"\"Adiciona um documento à coleção Solr especificada.\n",
    "\n",
    "    Args:\n",
    "        collection_name (str): O nome da coleção onde o documento será adicionado.\n",
    "        document (dict): Dicionário contendo os campos e valores do documento.\n",
    "\n",
    "    Example:\n",
    "        document = {\n",
    "            \"title\": \"Example Document\",\n",
    "            \"content\": \"This is an example document.\",\n",
    "            \"author\": \"John Doe\"\n",
    "        }\n",
    "        add_document_to_solr(\"my_collection\", document)\n",
    "\n",
    "    \"\"\"\n",
    "    # Converter o caminho do arquivo em string\n",
    "    document = {k: str(v) if isinstance(v, Path) else v for k, v in document.items()}\n",
    "\n",
    "    # URL da API do Solr para adicionar documentos\n",
    "    solr_url = f\"http://localhost:8983/solr/{collection_name}/update?commit=true\"\n",
    "\n",
    "    # Envia a requisição POST para adicionar o documento\n",
    "    response = requests.post(solr_url, json=[document])\n",
    "\n",
    "    # Verifica o status da resposta\n",
    "    status_ok = 200\n",
    "    if response.status_code == status_ok:\n",
    "        print(\"Documento adicionado com sucesso!\")\n",
    "    else:\n",
    "        print(\"Erro ao adicionar o documento:\", response.text)\n",
    "        print(response.content.decode())\n",
    "\n",
    "\n",
    "def pipeline(empresa, ano):\n",
    "    \"\"\"Executa o pipeline de captura,\n",
    "    preprocessamento e indexação de documentos econômico-financeiros.\n",
    "\n",
    "    Args:\n",
    "        empresa (str): O nome da empresa para o qual deseja-se executar o pipeline.\n",
    "        ano (int): O ano para o qual deseja-se executar o pipeline.\n",
    "\n",
    "    Returns:\n",
    "        str: Uma mensagem indicando o resultado da atualização do esquema no Solr.\n",
    "\n",
    "    Example:\n",
    "        >>> pipeline(\"Empresa XYZ\", 2023)\n",
    "        Inicio do pipeline de captura *********\n",
    "        Empresa encontrada: Empresa XYZ | codigo cvm: 123456\n",
    "        Ano: 2023\n",
    "        nome do arquivo: 123456_Empresa_XYZ\n",
    "        caminho dos arquivos: /caminho/do/diretorio/123456_Empresa_XYZ\n",
    "        diretorio não existe e será criado\n",
    "        diretorio não existe e será criado\n",
    "        diretorio não existe e será criado\n",
    "        Download finalizado. Arquivo salvo em:\n",
    "        /caminho/do/dir/123_Empresa_XYZ/2023/Economic_Fin_Data/123456_Empresa_XYZ.pdf\n",
    "        geração de token e tensor.....\n",
    "        Documento adicionado com sucesso!\n",
    "        ...\n",
    "        1/2 Esquema atualizado com sucesso.\n",
    "        2/2 Commit realizado com sucesso.\n",
    "    \"\"\"  # noqa: D205\n",
    "    # Faz o download do documento\n",
    "    root = verificar_diretorio()\n",
    "    collection_name = \"dados_eco\"\n",
    "    print(\"Inicio do pipeline de captura *********\")\n",
    "    doc = download_def(empresa, ano, root)\n",
    "    print(\"Inicio do pipeline de preprocessamento e indexacao *********\")\n",
    "    for item in doc:\n",
    "        # Define o tipo do item como \"document\"\n",
    "        item[\"type\"] = \"document\"\n",
    "        # Define o nome da coleção como \"dados_eco\"\n",
    "        collection_name = \"dados_eco\"\n",
    "        # Obtém o diretório do arquivo\n",
    "        diretorio = Path(item[\"file\"]).parent\n",
    "        # Obtém o nome do arquivo sem a extensão\n",
    "        file_path = Path(item[\"file\"])\n",
    "        tokens_tensor = file_path.stem, file_path.suffix\n",
    "        # Define o caminho do arquivo de tokens e tensor\n",
    "        dataprep = Path(diretorio) / f\"{tokens_tensor}_TOKENS_TENSOR.json\"\n",
    "        # Cria o output data com os tokens e tensor\n",
    "        text = convert_pdf(item[\"file\"])\n",
    "        #token = tokengen(text)\n",
    "        token = \"teste pipeline completo\"\n",
    "        #vector = vector_one(text).tolist()\n",
    "        vector=\"AGOOOORA VAIIIIIII\"\n",
    "        output_data = [{\n",
    "            \"tokens\": token,\n",
    "            \"tensor\": vector,\n",
    "        }]\n",
    "\n",
    "        # Salva o output data em um arquivo JSON\n",
    "        with dataprep.open(\"w\") as file:\n",
    "            json.dump(output_data, file)\n",
    "\n",
    "        # Atualiza os campos tokens e tensor do item com o caminho do arquivo\n",
    "        item[\"tokens\"] = dataprep\n",
    "        item[\"tensor\"] = dataprep\n",
    "        # Verifica se algum campo está ausente e define como\n",
    "        # \"Descrição ausente\" se necessário\n",
    "        fields = [\"name\", \"type\", \"company_name\", \"cod_cvm\", \"content\", \"year\",\n",
    "                  \"date\", \"keywords\", \"size\", \"tokens\", \"tensor\", \"file\"]\n",
    "        for field in fields:\n",
    "            if not item[field]:\n",
    "                item[field] = \"Descrição ausente\"\n",
    "        # Adiciona o documento ao Solr\n",
    "        add_document_to_solr(collection_name, item)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thgcn\\OneDrive\\Academico\\PO-245\\Projeto\\PO245\\po_245_2023_T4\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Some weights of the model checkpoint at allenai/longformer-large-4096 were not used when initializing LongformerModel: ['lm_head.decoder.weight', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing LongformerModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing LongformerModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "c:\\Users\\thgcn\\OneDrive\\Academico\\PO-245\\Projeto\\PO245\\po_245_2023_T4\\.venv\\Lib\\site-packages\\transformers\\models\\t5\\tokenization_t5.py:163: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Este módulo contém funções e bibliotecas relacionadas ao pipeline de captura,\n",
    "pré-processamento e indexação de relatórios.\n",
    "\"\"\"  # noqa: D205\n",
    "\n",
    "import io\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import subprocess\n",
    "import zipfile\n",
    "from pathlib import Path\n",
    "\n",
    "import PyPDF2\n",
    "import requests\n",
    "import spacy\n",
    "import torch\n",
    "import torch.nn.functional as fun\n",
    "import unidecode\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from transformers import (\n",
    "    BertForQuestionAnswering,\n",
    "    LongformerModel,\n",
    "    LongformerTokenizer,\n",
    "    T5ForConditionalGeneration,\n",
    "    T5Tokenizer,\n",
    ")\n",
    "\n",
    "model_for_question = BertForQuestionAnswering.from_pretrained(\n",
    "    \"bert-large-uncased-whole-word-masking-finetuned-squad\",\n",
    ")\n",
    "tokenizer = LongformerTokenizer.from_pretrained(\"allenai/longformer-large-4096\")\n",
    "model = LongformerModel.from_pretrained(\"allenai/longformer-large-4096\")\n",
    "model_sum = T5ForConditionalGeneration.from_pretrained(\"t5-base\")\n",
    "tokenizer_sum = T5Tokenizer.from_pretrained(\"t5-base\")\n",
    "\n",
    "def verificar_diretorio():\n",
    "    \"\"\"Verifica se o diretório informado pelo usuário é válido.\n",
    "\n",
    "    Returns:\n",
    "        str: O caminho absoluto do diretório válido.\n",
    "\n",
    "    Example:\n",
    "        >>> verificar_diretorio()\n",
    "        Digite o caminho do diretório: /caminho/do/diretorio\n",
    "        Diretório raiz para armazenamento dos documentos: /caminho/do/diretorio\n",
    "        '/caminho/do/diretorio'\n",
    "\n",
    "    \"\"\"\n",
    "    root = Path(input(\"Digite o caminho do diretório: \"))\n",
    "    try:\n",
    "        if root.is_dir():\n",
    "            print(\"Diretório raiz para armazenamento dos documentos: \" + str(root))\n",
    "            return root\n",
    "    except FileNotFoundError:\n",
    "        print(\"O diretório inserido não existe ou não é válido. \\\n",
    "            Certifique-se que o nome está correto\")\n",
    "\n",
    "def busca_ipe(ano):\n",
    "    \"\"\"Verifica se o diretório informado pelo usuário é válido.\n",
    "\n",
    "    Returns:\n",
    "        str: O caminho absoluto do diretório válido.\n",
    "\n",
    "    Example:\n",
    "        >>> verificar_diretorio()\n",
    "        Digite o caminho do diretório: /caminho/do/diretorio\n",
    "        Diretório raiz para armazenamento dos documentos: /caminho/do/diretorio\n",
    "        '/caminho/do/diretorio'\n",
    "\n",
    "    \"\"\"\n",
    "    url = \"https://dados.cvm.gov.br/dados/CIA_ABERTA/DOC/IPE/DADOS/\"\n",
    "    url += \"ipe_cia_aberta_%d.zip\" % ano\n",
    "    file = \"ipe_cia_aberta_%d.zip\" % ano\n",
    "    r = requests.get(url)\n",
    "    zf = zipfile.ZipFile(io.BytesIO(r.content))\n",
    "    file = zf.namelist()\n",
    "    zf = zf.open(file[0])\n",
    "    lines = zf.readlines()\n",
    "    lines = [i.strip().decode(\"ISO-8859-1\") for i in lines]\n",
    "    lines = [i.split(\";\") for i in lines]\n",
    "    return lines\n",
    "\n",
    "def search(lista, valor):\n",
    "    \"\"\"Retorna uma lista de elementos da lista que contêm o valor especificado.\n",
    "\n",
    "    Args:\n",
    "        lista (list): A lista de elementos para realizar a busca.\n",
    "        valor: O valor a ser procurado nos elementos da lista.\n",
    "\n",
    "    Returns:\n",
    "        list: Uma lista dos elementos que contêm o valor especificado.\n",
    "\n",
    "    Example:\n",
    "        >>> search(['apple', 'banana', 'orange'], 'an')\n",
    "        ['banana', 'orange']\n",
    "\n",
    "    \"\"\"\n",
    "    return [(lista[lista.index(x)]) for x in lista if valor in x]\n",
    "\n",
    "\n",
    "def baixar_arquivo(url, endereco):\n",
    "    \"\"\"Faz o download de um arquivo a partir de uma URL e salva no caminho especificado.\n",
    "\n",
    "    Args:\n",
    "        url (str): A URL do arquivo a ser baixado.\n",
    "        endereco (str): O caminho completo de destino para salvar o arquivo.\n",
    "\n",
    "    Raises:\n",
    "        Exception: Se ocorrer um erro ao fazer o download do arquivo.\n",
    "\n",
    "    Example:\n",
    "        >>> baixar_arquivo('https://example.com/file.pdf', '/path/to/save/file.pdf')\n",
    "        Download finalizado. Arquivo salvo em: /path/to/save/file.pdf\n",
    "\n",
    "    \"\"\"\n",
    "    resposta = requests.get(url, allow_redirects=True, verify=False, stream=True)\n",
    "    if resposta.status_code == requests.codes.OK:\n",
    "        with endereco.open(\"wb\") as novo_arquivo:\n",
    "            novo_arquivo.write(resposta.content)\n",
    "        print(f\"Download finalizado. Arquivo salvo em: {endereco}\")\n",
    "    else:\n",
    "        resposta.raise_for_status()\n",
    "\n",
    "\n",
    "def transform_string(text):\n",
    "    \"\"\"Remove acentos, substitui espaços por underline, converte para letras minúsculas.\n",
    "\n",
    "    Args:\n",
    "        text (str): A string a ser transformada.\n",
    "\n",
    "    Returns:\n",
    "        str: A string transformada.\n",
    "\n",
    "    Example:\n",
    "        >>> transform_string(\"Olá, Mundo!\")\n",
    "        'ola_mundo'\n",
    "\n",
    "    \"\"\"\n",
    "    text = unidecode.unidecode(text)\n",
    "    text = text.replace(\" \", \"_\")\n",
    "    text = text.replace(\".\", \"\")\n",
    "    text = text.lower()\n",
    "    return text\n",
    "\n",
    "\n",
    "def download_def(empresa, year, root):  # noqa: C901, PLR0915\n",
    "    \"\"\"Realiza download de arquivos específicos com base na empresa e no ano fornecidos.\n",
    "\n",
    "    Args:\n",
    "        empresa (str): Nome da empresa para qual deseja fazer o download dos arquivos.\n",
    "        year (int): O ano para o qual deseja-se fazer o download dos arquivos.\n",
    "        root(str):  Caminho onde serao armazenados os documentos\n",
    "\n",
    "    Returns:\n",
    "        list: Uma lista de dicionários contendo os metadados dos arquivos baixados.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: Se houver duplicidade de empresas encontradas.\n",
    "\n",
    "    Example:\n",
    "        >>> download_def(\"Empresa XYZ\", 2023)\n",
    "        Empresa encontrada: Empresa XYZ | codigo cvm: 12345\n",
    "        Ano: 2023\n",
    "        nome do arquivo: 12345_empresa_xyz\n",
    "        caminho dos arquivos: /caminho/do/arquivo/12345_empresa_xyz\n",
    "        ...\n",
    "\n",
    "    \"\"\"\n",
    "    lines = busca_ipe(year)\n",
    "    defi = search(lines, \"Dados Econômico-Financeiros\")\n",
    "    data = []\n",
    "    empresa_name = \"\"\n",
    "    num_cvm = \"\"\n",
    "    for a in range(len(defi)):\n",
    "        if empresa.upper() in defi[a][1]: \n",
    "            empresa_name = defi[a][1]\n",
    "            num_cvm = defi[a][2]\n",
    "            print(\"Empresa encontrada: \" + empresa_name + \" | codigo cvm: \" + num_cvm)\n",
    "            print(\"Ano: \" + str(year))\n",
    "            sufix = transform_string(defi[a][2] + \"_\" + defi[a][1])\n",
    "            company = root / sufix\n",
    "            category = transform_string(defi[a][5])\n",
    "            print(\"nome do arquivo: \" + sufix)\n",
    "            print(\"caminho dos arquivos: \" + str(company))\n",
    "            if not company.is_dir():\n",
    "                print(\"diretorio não existe e será criado\")\n",
    "                company.mkdir(parents=True)\n",
    "            diryear = company / str(year)\n",
    "            if not diryear.is_dir():\n",
    "                print(\"diretorio não existe e será criado\")\n",
    "                diryear.mkdir(parents=True)\n",
    "            dircategory = diryear / category\n",
    "            if not dircategory.is_dir():\n",
    "                print(\"diretorio não existe e será criado\")\n",
    "                dircategory.mkdir(parents=True)\n",
    "            url = defi[a][12]\n",
    "            nome = (\n",
    "                transform_string(defi[a][2])\n",
    "                + \"_\"\n",
    "                + defi[a][1]\n",
    "                + \"_\"\n",
    "                + defi[a][7][1:50]\n",
    "                + \"_\"\n",
    "                + defi[a][8]\n",
    "            )[1:100]\n",
    "\n",
    "            baixar_arquivo(url, dircategory / f\"{nome}.pdf\")\n",
    "\n",
    "            title = transform_string(defi[a][6][0:50])\n",
    "            if not title:\n",
    "                if not defi[a][7]:\n",
    "                    title = defi[a][7]\n",
    "                elif not defi[a][5]:\n",
    "                    title = defi[a][5]\n",
    "                elif not defi[a][4]:\n",
    "                    title = defi[a][4]\n",
    "                else:\n",
    "                    title = defi[a][1]\n",
    "            company_name = transform_string(defi[a][1])\n",
    "            cod_cvm = defi[a][2]\n",
    "            date = defi[a][3]\n",
    "            content = defi[a][7]\n",
    "            if not content:\n",
    "                content = defi[a][6]\n",
    "            keyword = defi[a][4]\n",
    "            file_path = dircategory / f\"{nome}.pdf\"\n",
    "            size = file_path.stat().st_size\n",
    "            file = str(file_path)\n",
    "            data.append({\n",
    "                \"name\": title,\n",
    "                \"type\": \"string\",\n",
    "                \"company_name\": company_name,\n",
    "                \"cod_cvm\": cod_cvm,\n",
    "                \"content\": content,\n",
    "                \"year\": year,\n",
    "                \"date\": date,\n",
    "                \"keywords\": keyword,\n",
    "                \"size\": size,\n",
    "                \"tokens\": \"\",\n",
    "                \"tensor\": \"\",\n",
    "                \"file\": file,\n",
    "            })\n",
    "    return data\n",
    "\n",
    "\n",
    "def convert_pdf(doc):\n",
    "    \"\"\"Converte um arquivo PDF em texto.\n",
    "\n",
    "    Args:\n",
    "        doc (str): O caminho do arquivo PDF a ser convertido.\n",
    "\n",
    "    Returns:\n",
    "        str: O texto extraído do arquivo PDF.\n",
    "\n",
    "    Example:\n",
    "        text = convert_pdf(\"path/to/file.pdf\")\n",
    "        print(text)\n",
    "        # Output: Text extracted from the PDF file.\n",
    "\n",
    "    \"\"\"\n",
    "    with Path(doc).open(\"rb\") as f:\n",
    "        # Use a biblioteca PyPDF2 para ler o arquivo\n",
    "        reader = PyPDF2.PdfReader(f)\n",
    "        # Obtenha o número de páginas no arquivo\n",
    "        num_pages = len(reader.pages)\n",
    "        # String vazia para armazenar o texto extraído\n",
    "        text = \"\"\n",
    "        # Itere pelas páginas e extraia o texto\n",
    "        for page in range(num_pages):\n",
    "            # Obtenha o objeto da página\n",
    "            # page_obj = reader.getPage(page)\n",
    "            page_obj = reader.pages[page]\n",
    "            # Extraia o texto da página\n",
    "            # page_text = page_obj.extractText()\n",
    "            page_text = page_obj.extract_text()\n",
    "            # Adicione o texto extraído à string de texto\n",
    "            text += page_text\n",
    "    return text\n",
    "\n",
    "def remover_stopwords(tokens):\n",
    "    \"\"\"Remove as stopwords de uma lista de tokens.\n",
    "\n",
    "    Args:\n",
    "        tokens (list): A lista de tokens.\n",
    "\n",
    "    Returns:\n",
    "        list: A lista de tokens sem as stopwords.\n",
    "\n",
    "    Example:\n",
    "        token_list = ['This', 'is', 'a', 'sample', 'sentence']\n",
    "        tokens_without_stopwords = remover_stopwords(token_list)\n",
    "        print(tokens_without_stopwords)\n",
    "        # Output: ['This', 'sample', 'sentence']\n",
    "\n",
    "    \"\"\"\n",
    "    # Carrega o modelo do SpaCy para o idioma português\n",
    "    nlp = spacy.load(\"pt_core_news_lg\")\n",
    "\n",
    "    # Cria uma lista para armazenar os tokens sem stopwords\n",
    "    tokens_sem_stopwords = []\n",
    "\n",
    "    # Percorre cada token na lista\n",
    "    for token in tokens:\n",
    "        # Verifica se o token não é uma stopword\n",
    "        if not nlp.vocab[token].is_stop:\n",
    "            tokens_sem_stopwords.append(token)\n",
    "    return tokens_sem_stopwords\n",
    "\n",
    "\n",
    "def normalize_text(text):\n",
    "    \"\"\"Normaliza o texto, convertendo-o para minúsculas e removendo caracteres especiais\n",
    "    e acentuação.\n",
    "\n",
    "    Args:\n",
    "        text (str): O texto a ser normalizado.\n",
    "\n",
    "    Returns:\n",
    "        str: O texto normalizado.\n",
    "\n",
    "    Example:\n",
    "        text = \"Hello, World! This is an example text.\"\n",
    "        normalized_text = normalize_text(text)\n",
    "        print(normalized_text)\n",
    "        # Output: hello world this is an example text\n",
    "\n",
    "    \"\"\"  # noqa: D205\n",
    "    # Converte o texto para minúsculas\n",
    "    normalized_text = text.lower()\n",
    "    # Remove caracteres especiais e acentuação\n",
    "    normalized_text = re.sub(r\"[^\\w\\s]\", \"\", normalized_text)\n",
    "    # Retorna o texto limpo\n",
    "    return normalized_text\n",
    "\n",
    "def tokengen(text):\n",
    "    \"\"\"Gera uma lista de tokens a partir de um texto.\n",
    "\n",
    "    Args:\n",
    "        text (str): O texto a ser tokenizado.\n",
    "\n",
    "    Returns:\n",
    "        list: Uma lista de tokens.\n",
    "\n",
    "    Example:\n",
    "        text = \"Hello, World! This is an example text.\"\n",
    "        tokens = tokengen(text)\n",
    "        print(tokens)\n",
    "        # Output: ['[CLS]', 'hello', ',', 'world', '!', 'this', 'is', 'an', 'example',\n",
    "        # 'text', '.', '[SEP]']\n",
    "\n",
    "    \"\"\"\n",
    "    tokens = tokenizer.encode(text, add_special_tokens=True)\n",
    "    return tokenizer.convert_ids_to_tokens(tokens)\n",
    "\n",
    "def vector_one(text):\n",
    "    \"\"\"Gera um vetor de representação para um texto.\n",
    "\n",
    "    Args:\n",
    "        text (str): O texto a ser vetorizado.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: O vetor de representação do texto.\n",
    "\n",
    "    Example:\n",
    "        text = \"This is a sample text.\"\n",
    "        text_vector = vector_one(text)\n",
    "        print(text_vector)\n",
    "        # Output: torch.Tensor representing the vector for the text.\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    tokens = tokenizer.encode(text, add_special_tokens=True)\n",
    "    input_ids = torch.tensor(tokens).unsqueeze(0)\n",
    "\n",
    "    max_length = 4096\n",
    "    vectors = []\n",
    "    # Divide the tokenized text into parts\n",
    "    input_parts = [\n",
    "    input_ids[:, i:i+max_length]\n",
    "    for i in range(0, input_ids.shape[1], max_length)\n",
    "    ]\n",
    "    # Process each part of the tokenized text\n",
    "    for part in input_parts:\n",
    "        # Generate the embeddings vectors\n",
    "        with torch.no_grad():\n",
    "            outputs = model(part)\n",
    "            last_hidden_states = outputs.last_hidden_state\n",
    "\n",
    "        # Calculate the average vector\n",
    "        average_vector = torch.mean(last_hidden_states, dim=1)\n",
    "\n",
    "        # Add the average vector to the list\n",
    "        vectors.append(average_vector)\n",
    "\n",
    "    # Concatenate the generated vectors\n",
    "    return torch.cat(vectors, dim=1)\n",
    "\n",
    "def similarity_vector(a, b):\n",
    "    \"\"\"Calcula a similaridade entre dois vetores.\n",
    "\n",
    "    Args:\n",
    "        a (torch.Tensor): O primeiro vetor.\n",
    "        b (torch.Tensor): O segundo vetor.\n",
    "\n",
    "    Returns:\n",
    "        float: O valor da similaridade entre os vetores.\n",
    "\n",
    "    Example:\n",
    "        vector1 = torch.tensor([0.1, 0.2, 0.3])\n",
    "        vector2 = torch.tensor([0.4, 0.5, 0.6])\n",
    "        similarity = similarity_vector(vector1, vector2)\n",
    "        print(similarity)\n",
    "        # Output: Similarity score between the vectors.\n",
    "\n",
    "    \"\"\"\n",
    "    size_a = a.size(0)\n",
    "    size_b = b.size(0)\n",
    "    if size_a == size_b:\n",
    "        # similarity_score = cosine_similarity(a, b)\n",
    "        similarity_score = cosine_similarity(a.float(), b.float())\n",
    "        return similarity_score.item()\n",
    "    if size_a > size_b:\n",
    "        diff = size_a - size_b\n",
    "        padded = fun.pad(b, pad=(0, diff), mode=\"constant\", value=0)\n",
    "        similarity_score = cosine_similarity(a.float(), padded.float())\n",
    "        return similarity_score.item()\n",
    "    if size_a < size_b:\n",
    "        diff = size_b - size_a\n",
    "        padded = fun.pad(a, pad=(0, diff), mode=\"constant\", value=0)\n",
    "        similarity_score = cosine_similarity(b.float(), padded.float())\n",
    "        return similarity_score.item()\n",
    "    return None\n",
    "\n",
    "def question(text, question):\n",
    "    \"\"\"Calcula a resposta para uma pergunta com base em um texto usando\n",
    "    um modelo de similaridade.\n",
    "\n",
    "    Args:\n",
    "        text (str): O texto em que a pergunta será feita.\n",
    "        question (str): A pergunta a ser respondida.\n",
    "\n",
    "    Returns:\n",
    "        str: A resposta para a pergunta.\n",
    "\n",
    "    Example:\n",
    "        text = \"Lorem ipsum dolor sit amet, consectetur adipiscing elit.\"\n",
    "        question = \"Qual é o significado de Lorem ipsum?\"\n",
    "        answer = question(text, question)\n",
    "        print(answer)\n",
    "        # Output: Resposta para a pergunta..\n",
    "\n",
    "    \"\"\"  # noqa: D205\n",
    "    question = question\n",
    "    max_answer_length = 512\n",
    "    max_length = 512\n",
    "    input_parts = []\n",
    "    input_parts = [text[i:i+max_length] for i in range(0, len(text), max_length)]\n",
    "    # Lista para armazenar as respostas\n",
    "    answers = []\n",
    "    # Processa cada parte do texto\n",
    "    for part in input_parts:\n",
    "        # Tokeniza a parte do texto\n",
    "        # Tokeniza a pergunta e o texto menor\n",
    "        encoding = tokenizer.encode_plus(question, part, return_tensors=\"pt\",\n",
    "                                         max_length=512, truncation=True)\n",
    "        input_ids = encoding[\"input_ids\"]\n",
    "        attention_mask = encoding[\"attention_mask\"]\n",
    "        # Obtém as respostas do modelo pré-treinado\n",
    "        with torch.no_grad():\n",
    "            outputs = model_for_question(\n",
    "                input_ids=input_ids, attention_mask=attention_mask)\n",
    "            start_scores = outputs.start_logits\n",
    "            end_scores = outputs.end_logits\n",
    "        # Obtém a resposta com maior probabilidade\n",
    "        answer_start = torch.argmax(start_scores)\n",
    "        answer_end = torch.argmax(end_scores)\n",
    "        answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(\n",
    "            input_ids[0][answer_start.item():answer_end.item()+1]))\n",
    "        # Adiciona a resposta à lista\n",
    "        answers.append(answer)\n",
    "    # Combina as respostas\n",
    "    combined_answer = ' '.join(answers)\n",
    "    # Limita o tamanho da resposta final\n",
    "    combined_answer = combined_answer[:max_answer_length]\n",
    "        # Formatação da resposta final\n",
    "    formatted_answer = \"RESPOSTA:\\n\\n\"\n",
    "    formatted_answer += combined_answer.replace(\".\", \".\\n\\n\")\n",
    "    # Apresenta a resposta final\n",
    "    return formatted_answer\n",
    "\n",
    "def summarization(text):\n",
    "    \"\"\"Gera um resumo do texto fornecido.\n",
    "\n",
    "    Args:\n",
    "        text (str): O texto a ser resumido.\n",
    "\n",
    "    Returns:\n",
    "        str: O resumo gerado do texto.\n",
    "\n",
    "    Example:\n",
    "        text = \"This is a sample text. It contains multiple sentences.\n",
    "        The goal is to generate a summary.\"\n",
    "        summary = summarization(text)\n",
    "        print(summary)\n",
    "        # Output: A summary of the text.\n",
    "\n",
    "    \"\"\"\n",
    "    num_sentences = 5\n",
    "    # Tokenizar o texto em partes\n",
    "    input_parts = text.split(\". \")\n",
    "    summaries = []\n",
    "    for part in input_parts:\n",
    "        length = len(part)\n",
    "        m_length = int(length * 0.4)\n",
    "        # Tokenizar a parte do texto\n",
    "        inputs = tokenizer_sum.encode(\"summarize: \" + part, return_tensors=\"pt\")\n",
    "        # Gerar o resumo da parte do texto usando o modelo T5\n",
    "        outputs = model_sum.generate(\n",
    "            inputs, max_length=m_length, num_beams=4, early_stopping=True)\n",
    "        summary = tokenizer_sum.decode(outputs[0], skip_special_tokens=True)\n",
    "        # Adicionar o resumo à lista de sumários\n",
    "        summaries.append(summary)\n",
    "\n",
    "    # Concatenar os sumários em um único texto\n",
    "    concatenated_summary = \" \".join(summaries)\n",
    "\n",
    "    # Extrair as N sentenças mais importantes\n",
    "    sentences = concatenated_summary.split(\". \")\n",
    "    top_sentences = sorted(\n",
    "        sentences, key=lambda x: len(x), reverse=True)[:num_sentences]\n",
    "\n",
    "    return \". \".join(top_sentences)\n",
    "\n",
    "def start_solr_service(solr_bin_path):\n",
    "    \"\"\"Inicia o serviço do Solr.\n",
    "\n",
    "    Args:\n",
    "        solr_bin_path (str): O caminho para o diretório bin do Solr.\n",
    "\n",
    "    Returns:\n",
    "        int: O código de retorno da execução do comando.\n",
    "    \"\"\"\n",
    "    # Comando para iniciar o serviço do Solr\n",
    "    command = f\"{solr_bin_path}solr start -port 8983\"\n",
    "\n",
    "    # Executa o comando no terminal\n",
    "    result = subprocess.run(command, shell=True)\n",
    "\n",
    "    # Retorna o código de retorno da execução do comando\n",
    "    return result.returncode\n",
    "\n",
    "\n",
    "def check_collection_exists(collection_name):\n",
    "    \"\"\"Verifica se uma coleção existe no servidor Solr.\n",
    "\n",
    "    Args:\n",
    "        collection_name (str): O nome da coleção a ser verificada.\n",
    "\n",
    "    Returns:\n",
    "        bool: True se a coleção existe, False caso contrário.\n",
    "\n",
    "    Example:\n",
    "        >>> check_collection_exists(\"my_collection\")\n",
    "        True\n",
    "\n",
    "    \"\"\"\n",
    "    url = f\"http://localhost:8983/solr/{collection_name}/admin/ping\"\n",
    "\n",
    "    response = requests.get(url)\n",
    "    status_ok = 200\n",
    "    status_not_found = 404\n",
    "    if response.status_code == status_ok:\n",
    "        return True\n",
    "    elif response.status_code == status_not_found:  # noqa: RET505\n",
    "        return False\n",
    "    else:\n",
    "        print(\"Falha ao verificar a existência da coleção. Status:\",\n",
    "              response.status_code)\n",
    "        return False\n",
    "\n",
    "def create_solr_collection(collection_name):\n",
    "    \"\"\"Cria uma coleção no servidor Solr e atualiza o esquema.\n",
    "\n",
    "    Args:\n",
    "        collection_name (str): O nome da coleção a ser criada.\n",
    "\n",
    "    Example:\n",
    "        >>> create_solr_collection(\"my_collection\")\n",
    "        Esquema atualizado com sucesso.\n",
    "\n",
    "    \"\"\"\n",
    "    # Caminho para o diretório bin do Solr\n",
    "    solr = \"C:\\\\Users\\\\thgcn\\\\OneDrive\\\\Academico\\\\PO-245\\\\Projeto\\\\solr-9.2.1\\\\bin\\\\\"\n",
    "\n",
    "    # Mude para o diretório bin do Solr\n",
    "    os.chdir(solr)\n",
    "\n",
    "    # Comando para criar a coleção no modo standalone\n",
    "    create_collection_command = f\"solr.cmd create -c {collection_name} -s 2 -rf 2\"\n",
    "\n",
    "    # Execute o comando no terminal\n",
    "    subprocess.run(create_collection_command, shell=True)\n",
    "\n",
    "    # URL do endpoint Solr\n",
    "    url = f\"http://localhost:8983/solr/{collection_name}/schema\"\n",
    "\n",
    "    # Cabeçalhos da solicitação POST\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\",\n",
    "    }\n",
    "\n",
    "    # Corpo da solicitação POST\n",
    "    data = {\n",
    "        \"add-field\": [\n",
    "            {\"name\": \"name\", \"type\": \"text_general\", \"multiValued\": False},\n",
    "            {\"name\": \"company_name\", \"type\": \"text_general\", \"multiValued\": False},\n",
    "            {\"name\": \"cod_cvm\", \"type\": \"text_general\", \"multiValued\": False},\n",
    "            {\"name\": \"content\", \"type\": \"text_general\", \"multiValued\": True},\n",
    "            {\"name\": \"year\", \"type\": \"pint\"},\n",
    "            {\"name\": \"date\", \"type\": \"pdate\"},\n",
    "            {\"name\": \"keywords\", \"type\": \"text_general\", \"multiValued\": True},\n",
    "            {\"name\": \"size\", \"type\": \"pint\"},\n",
    "            {\"name\": \"tokens\", \"type\": \"text_general\", \"multiValued\": False},\n",
    "            {\"name\": \"tensor\", \"type\": \"text_general\", \"multiValued\": False},\n",
    "            {\"name\": \"file\", \"type\": \"text_general\", \"multiValued\": False}\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    # Enviar solicitação POST para atualizar o esquema no Solr\n",
    "    response = requests.post(url, json=data, headers=headers)\n",
    "    status_ok = 200\n",
    "\n",
    "    if response.status_code == status_ok:\n",
    "        print(\"Esquema atualizado com sucesso.\")\n",
    "    else:\n",
    "        print(\"Falha ao atualizar o esquema. Status:\", response.status_code)\n",
    "\n",
    "def update_schema(data, collection_name):\n",
    "    \"\"\"Atualiza o esquema da coleção Solr com os campos fornecidos.\n",
    "\n",
    "    Args:\n",
    "        data (dict): Dicionário contendo os campos e configurações\n",
    "        a serem adicionados ao esquema.\n",
    "        collection_name (str): O nome da coleção a ser atualizada.\n",
    "\n",
    "    Example:\n",
    "        data = {\n",
    "            \"add-field\": [\n",
    "                {\"name\": \"title\", \"type\": \"text_general\", \"multiValued\": False},\n",
    "                {\"name\": \"content\", \"type\": \"text_general\", \"multiValued\": True},\n",
    "                {\"name\": \"author\", \"type\": \"text_general\", \"multiValued\": False}\n",
    "            ]\n",
    "        }\n",
    "        update_schema(data, \"my_collection\")\n",
    "\n",
    "    \"\"\"\n",
    "    # URL do endpoint Solr\n",
    "    url = f\"http://localhost:8983/solr/{collection_name}/schema\"\n",
    "\n",
    "    # Cabeçalhos da solicitação POST\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\"}\n",
    "\n",
    "    # Converter tensores em listas\n",
    "    data = json.loads(json.dumps(data, default=lambda x: x.tolist()))\n",
    "\n",
    "    # Enviar solicitação POST para atualizar o esquema no Solr\n",
    "    response = requests.post(url, json=data, headers=headers)\n",
    "\n",
    "    upd = f\"http://localhost:8983/solr/{collection_name}/config\"\n",
    "    status_ok = 200\n",
    "    if response.status_code == status_ok:\n",
    "        print(\"1/2 Esquema atualizado com sucesso.\")\n",
    "        data_up = {\n",
    "        \"set-property\": {\n",
    "            \"updateHandler.autoCommit.maxTime\": 15000,\n",
    "            },\n",
    "        }\n",
    "        response = requests.post(upd, headers=headers, json=data_up)\n",
    "        if response.status_code == status_ok:\n",
    "            print(\"2/2 Commit realizado com sucesso.\")\n",
    "        else:\n",
    "            print(\"Falha no commit. Status:\", response.status_code)\n",
    "            print(response.content.decode())\n",
    "    else:\n",
    "        print(\"Falha ao atualizar o esquema. Status:\", response.status_code)\n",
    "        print(response.content.decode())\n",
    "\n",
    "def add_document_to_solr(collection_name, document):\n",
    "    \"\"\"Adiciona um documento à coleção Solr especificada.\n",
    "\n",
    "    Args:\n",
    "        collection_name (str): O nome da coleção onde o documento será adicionado.\n",
    "        document (dict): Dicionário contendo os campos e valores do documento.\n",
    "\n",
    "    Example:\n",
    "        document = {\n",
    "            \"title\": \"Example Document\",\n",
    "            \"content\": \"This is an example document.\",\n",
    "            \"author\": \"John Doe\"\n",
    "        }\n",
    "        add_document_to_solr(\"my_collection\", document)\n",
    "\n",
    "    \"\"\"\n",
    "    # Converter o caminho do arquivo em string\n",
    "    document = {k: str(v) if isinstance(v, Path) else v for k, v in document.items()}\n",
    "\n",
    "    # URL da API do Solr para adicionar documentos\n",
    "    solr_url = f\"http://localhost:8983/solr/{collection_name}/update?commit=true\"\n",
    "\n",
    "    # Envia a requisição POST para adicionar o documento\n",
    "    response = requests.post(solr_url, json=[document])\n",
    "\n",
    "    # Verifica o status da resposta\n",
    "    status_ok = 200\n",
    "    if response.status_code == status_ok:\n",
    "        print(\"Documento adicionado com sucesso!\")\n",
    "    else:\n",
    "        print(\"Erro ao adicionar o documento:\", response.text)\n",
    "        print(response.content.decode())\n",
    "\n",
    "\n",
    "def pipeline(empresa, ano):\n",
    "    \"\"\"Executa o pipeline de captura,\n",
    "    preprocessamento e indexação de documentos econômico-financeiros.\n",
    "\n",
    "    Args:\n",
    "        empresa (str): O nome da empresa para o qual deseja-se executar o pipeline.\n",
    "        ano (int): O ano para o qual deseja-se executar o pipeline.\n",
    "\n",
    "    Returns:\n",
    "        str: Uma mensagem indicando o resultado da atualização do esquema no Solr.\n",
    "\n",
    "    Example:\n",
    "        >>> pipeline(\"Empresa XYZ\", 2023)\n",
    "        Inicio do pipeline de captura *********\n",
    "        Empresa encontrada: Empresa XYZ | codigo cvm: 123456\n",
    "        Ano: 2023\n",
    "        nome do arquivo: 123456_Empresa_XYZ\n",
    "        caminho dos arquivos: /caminho/do/diretorio/123456_Empresa_XYZ\n",
    "        diretorio não existe e será criado\n",
    "        diretorio não existe e será criado\n",
    "        diretorio não existe e será criado\n",
    "        Download finalizado. Arquivo salvo em:\n",
    "        /caminho/do/dir/123_Empresa_XYZ/2023/Economic_Fin_Data/123456_Empresa_XYZ.pdf\n",
    "        geração de token e tensor.....\n",
    "        Documento adicionado com sucesso!\n",
    "        ...\n",
    "        1/2 Esquema atualizado com sucesso.\n",
    "        2/2 Commit realizado com sucesso.\n",
    "    \"\"\"  # noqa: D205\n",
    "    # Faz o download do documento\n",
    "    root = verificar_diretorio()\n",
    "    collection_name = \"dados_eco\"\n",
    "    print(\"Inicio do pipeline de captura *********\")\n",
    "    doc = download_def(empresa, ano, root)\n",
    "    print(\"Inicio do pipeline de preprocessamento e indexacao *********\")\n",
    "    for item in doc:\n",
    "        # Define o tipo do item como \"document\"\n",
    "        item[\"type\"] = \"document\"\n",
    "        # Define o nome da coleção como \"dados_eco\"\n",
    "        collection_name = \"dados_eco\"\n",
    "        # Obtém o diretório do arquivo\n",
    "        diretorio = Path(item[\"file\"]).parent\n",
    "        # Obtém o nome do arquivo sem a extensão\n",
    "        file_path = Path(item[\"file\"])\n",
    "        tokens_tensor = file_path.stem, file_path.suffix\n",
    "        # Define o caminho do arquivo de tokens e tensor\n",
    "        dataprep = Path(diretorio) / f\"{tokens_tensor}_TOKENS_TENSOR.json\"\n",
    "        # Cria o output data com os tokens e tensor\n",
    "        text = convert_pdf(item[\"file\"])\n",
    "        token = tokengen(text)\n",
    "        vector = vector_one(text).tolist()\n",
    "        output_data = [{\n",
    "            \"tokens\": token,\n",
    "            \"tensor\": vector,\n",
    "        }]\n",
    "\n",
    "        # Salva o output data em um arquivo JSON\n",
    "        with dataprep.open(\"w\") as file:\n",
    "            json.dump(output_data, file)\n",
    "\n",
    "        # Atualiza os campos tokens e tensor do item com o caminho do arquivo\n",
    "        item[\"tokens\"] = dataprep\n",
    "        item[\"tensor\"] = dataprep\n",
    "        # Verifica se algum campo está ausente e define como\n",
    "        # \"Descrição ausente\" se necessário\n",
    "        fields = [\"name\", \"type\", \"company_name\", \"cod_cvm\", \"content\", \"year\",\n",
    "                  \"date\", \"keywords\", \"size\", \"tokens\", \"tensor\", \"file\"]\n",
    "        for field in fields:\n",
    "            if not item[field]:\n",
    "                item[field] = \"Descrição ausente\"\n",
    "        # Adiciona o documento ao Solr\n",
    "        add_document_to_solr(collection_name, item)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diretório raiz para armazenamento dos documentos: C:\\Users\\thgcn\\OneDrive\\Academico\\PO-245\\Projeto\\PO245\n",
      "Inicio do pipeline de captura *********\n",
      "Empresa encontrada: MAGAZINE LUIZA SA | codigo cvm: 22470\n",
      "Ano: 2023\n",
      "nome do arquivo: 22470_magazine_luiza_sa\n",
      "caminho dos arquivos: C:\\Users\\thgcn\\OneDrive\\Academico\\PO-245\\Projeto\\PO245\\22470_magazine_luiza_sa\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thgcn\\OneDrive\\Academico\\PO-245\\Projeto\\PO245\\po_245_2023_T4\\.venv\\Lib\\site-packages\\urllib3\\connectionpool.py:1095: InsecureRequestWarning: Unverified HTTPS request is being made to host 'www.rad.cvm.gov.br'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download finalizado. Arquivo salvo em: C:\\Users\\thgcn\\OneDrive\\Academico\\PO-245\\Projeto\\PO245\\22470_magazine_luiza_sa\\2023\\demonstracoes_financeiras_anuais_completas\\2470_MAGAZINE LUIZA SA__2023-03-10.pdf\n",
      "Empresa encontrada: MAGAZINE LUIZA SA | codigo cvm: 22470\n",
      "Ano: 2023\n",
      "nome do arquivo: 22470_magazine_luiza_sa\n",
      "caminho dos arquivos: C:\\Users\\thgcn\\OneDrive\\Academico\\PO-245\\Projeto\\PO245\\22470_magazine_luiza_sa\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thgcn\\OneDrive\\Academico\\PO-245\\Projeto\\PO245\\po_245_2023_T4\\.venv\\Lib\\site-packages\\urllib3\\connectionpool.py:1095: InsecureRequestWarning: Unverified HTTPS request is being made to host 'www.rad.cvm.gov.br'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download finalizado. Arquivo salvo em: C:\\Users\\thgcn\\OneDrive\\Academico\\PO-245\\Projeto\\PO245\\22470_magazine_luiza_sa\\2023\\demonstracoes_financeiras_em_padroes_internacionais\\2470_MAGAZINE LUIZA SA_ersão em inglês_2023-03-21.pdf\n",
      "Empresa encontrada: MAGAZINE LUIZA SA | codigo cvm: 22470\n",
      "Ano: 2023\n",
      "nome do arquivo: 22470_magazine_luiza_sa\n",
      "caminho dos arquivos: C:\\Users\\thgcn\\OneDrive\\Academico\\PO-245\\Projeto\\PO245\\22470_magazine_luiza_sa\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thgcn\\OneDrive\\Academico\\PO-245\\Projeto\\PO245\\po_245_2023_T4\\.venv\\Lib\\site-packages\\urllib3\\connectionpool.py:1095: InsecureRequestWarning: Unverified HTTPS request is being made to host 'www.rad.cvm.gov.br'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download finalizado. Arquivo salvo em: C:\\Users\\thgcn\\OneDrive\\Academico\\PO-245\\Projeto\\PO245\\22470_magazine_luiza_sa\\2023\\demonstracoes_financeiras_em_padroes_internacionais\\2470_MAGAZINE LUIZA SA_ersão em inglês_2023-06-01.pdf\n",
      "Empresa encontrada: MAGAZINE LUIZA SA | codigo cvm: 22470\n",
      "Ano: 2023\n",
      "nome do arquivo: 22470_magazine_luiza_sa\n",
      "caminho dos arquivos: C:\\Users\\thgcn\\OneDrive\\Academico\\PO-245\\Projeto\\PO245\\22470_magazine_luiza_sa\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thgcn\\OneDrive\\Academico\\PO-245\\Projeto\\PO245\\po_245_2023_T4\\.venv\\Lib\\site-packages\\urllib3\\connectionpool.py:1095: InsecureRequestWarning: Unverified HTTPS request is being made to host 'www.rad.cvm.gov.br'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download finalizado. Arquivo salvo em: C:\\Users\\thgcn\\OneDrive\\Academico\\PO-245\\Projeto\\PO245\\22470_magazine_luiza_sa\\2023\\demonstracoes_financeiras_intermediarias\\2470_MAGAZINE LUIZA SA_emonstração Financeira Intermediária_2023-05-15.pdf\n",
      "Empresa encontrada: MAGAZINE LUIZA SA | codigo cvm: 22470\n",
      "Ano: 2023\n",
      "nome do arquivo: 22470_magazine_luiza_sa\n",
      "caminho dos arquivos: C:\\Users\\thgcn\\OneDrive\\Academico\\PO-245\\Projeto\\PO245\\22470_magazine_luiza_sa\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thgcn\\OneDrive\\Academico\\PO-245\\Projeto\\PO245\\po_245_2023_T4\\.venv\\Lib\\site-packages\\urllib3\\connectionpool.py:1095: InsecureRequestWarning: Unverified HTTPS request is being made to host 'www.rad.cvm.gov.br'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download finalizado. Arquivo salvo em: C:\\Users\\thgcn\\OneDrive\\Academico\\PO-245\\Projeto\\PO245\\22470_magazine_luiza_sa\\2023\\press-release\\2470_MAGAZINE LUIZA SA_elease Resultados 4T22 Português_2023-03-10.pdf\n",
      "Empresa encontrada: MAGAZINE LUIZA SA | codigo cvm: 22470\n",
      "Ano: 2023\n",
      "nome do arquivo: 22470_magazine_luiza_sa\n",
      "caminho dos arquivos: C:\\Users\\thgcn\\OneDrive\\Academico\\PO-245\\Projeto\\PO245\\22470_magazine_luiza_sa\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thgcn\\OneDrive\\Academico\\PO-245\\Projeto\\PO245\\po_245_2023_T4\\.venv\\Lib\\site-packages\\urllib3\\connectionpool.py:1095: InsecureRequestWarning: Unverified HTTPS request is being made to host 'www.rad.cvm.gov.br'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download finalizado. Arquivo salvo em: C:\\Users\\thgcn\\OneDrive\\Academico\\PO-245\\Projeto\\PO245\\22470_magazine_luiza_sa\\2023\\press-release\\2470_MAGAZINE LUIZA SA_elease Resultados 4Q22 Inglês_2023-03-10.pdf\n",
      "Empresa encontrada: MAGAZINE LUIZA SA | codigo cvm: 22470\n",
      "Ano: 2023\n",
      "nome do arquivo: 22470_magazine_luiza_sa\n",
      "caminho dos arquivos: C:\\Users\\thgcn\\OneDrive\\Academico\\PO-245\\Projeto\\PO245\\22470_magazine_luiza_sa\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thgcn\\OneDrive\\Academico\\PO-245\\Projeto\\PO245\\po_245_2023_T4\\.venv\\Lib\\site-packages\\urllib3\\connectionpool.py:1095: InsecureRequestWarning: Unverified HTTPS request is being made to host 'www.rad.cvm.gov.br'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download finalizado. Arquivo salvo em: C:\\Users\\thgcn\\OneDrive\\Academico\\PO-245\\Projeto\\PO245\\22470_magazine_luiza_sa\\2023\\press-release\\2470_MAGAZINE LUIZA SA_elatório da Administração 2022_2023-03-10.pdf\n",
      "Empresa encontrada: MAGAZINE LUIZA SA | codigo cvm: 22470\n",
      "Ano: 2023\n",
      "nome do arquivo: 22470_magazine_luiza_sa\n",
      "caminho dos arquivos: C:\\Users\\thgcn\\OneDrive\\Academico\\PO-245\\Projeto\\PO245\\22470_magazine_luiza_sa\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thgcn\\OneDrive\\Academico\\PO-245\\Projeto\\PO245\\po_245_2023_T4\\.venv\\Lib\\site-packages\\urllib3\\connectionpool.py:1095: InsecureRequestWarning: Unverified HTTPS request is being made to host 'www.rad.cvm.gov.br'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download finalizado. Arquivo salvo em: C:\\Users\\thgcn\\OneDrive\\Academico\\PO-245\\Projeto\\PO245\\22470_magazine_luiza_sa\\2023\\press-release\\2470_MAGAZINE LUIZA SA_elease de Resultados 1T23 - Português_2023-05-15.pdf\n",
      "Empresa encontrada: MAGAZINE LUIZA SA | codigo cvm: 22470\n",
      "Ano: 2023\n",
      "nome do arquivo: 22470_magazine_luiza_sa\n",
      "caminho dos arquivos: C:\\Users\\thgcn\\OneDrive\\Academico\\PO-245\\Projeto\\PO245\\22470_magazine_luiza_sa\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thgcn\\OneDrive\\Academico\\PO-245\\Projeto\\PO245\\po_245_2023_T4\\.venv\\Lib\\site-packages\\urllib3\\connectionpool.py:1095: InsecureRequestWarning: Unverified HTTPS request is being made to host 'www.rad.cvm.gov.br'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download finalizado. Arquivo salvo em: C:\\Users\\thgcn\\OneDrive\\Academico\\PO-245\\Projeto\\PO245\\22470_magazine_luiza_sa\\2023\\press-release\\2470_MAGAZINE LUIZA SA_elease de Resultados 1T23 - Inglês_2023-05-15.pdf\n",
      "Empresa encontrada: MAGAZINE LUIZA SA | codigo cvm: 22470\n",
      "Ano: 2023\n",
      "nome do arquivo: 22470_magazine_luiza_sa\n",
      "caminho dos arquivos: C:\\Users\\thgcn\\OneDrive\\Academico\\PO-245\\Projeto\\PO245\\22470_magazine_luiza_sa\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thgcn\\OneDrive\\Academico\\PO-245\\Projeto\\PO245\\po_245_2023_T4\\.venv\\Lib\\site-packages\\urllib3\\connectionpool.py:1095: InsecureRequestWarning: Unverified HTTPS request is being made to host 'www.rad.cvm.gov.br'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download finalizado. Arquivo salvo em: C:\\Users\\thgcn\\OneDrive\\Academico\\PO-245\\Projeto\\PO245\\22470_magazine_luiza_sa\\2023\\relatorio_de_agente_fiduciario\\2470_MAGAZINE LUIZA SA_ª emissão NP_2023-04-28.pdf\n",
      "Empresa encontrada: MAGAZINE LUIZA SA | codigo cvm: 22470\n",
      "Ano: 2023\n",
      "nome do arquivo: 22470_magazine_luiza_sa\n",
      "caminho dos arquivos: C:\\Users\\thgcn\\OneDrive\\Academico\\PO-245\\Projeto\\PO245\\22470_magazine_luiza_sa\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thgcn\\OneDrive\\Academico\\PO-245\\Projeto\\PO245\\po_245_2023_T4\\.venv\\Lib\\site-packages\\urllib3\\connectionpool.py:1095: InsecureRequestWarning: Unverified HTTPS request is being made to host 'www.rad.cvm.gov.br'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download finalizado. Arquivo salvo em: C:\\Users\\thgcn\\OneDrive\\Academico\\PO-245\\Projeto\\PO245\\22470_magazine_luiza_sa\\2023\\relatorio_de_agente_fiduciario\\2470_MAGAZINE LUIZA SA_ª emissão Debênture_2023-04-28.pdf\n",
      "Empresa encontrada: MAGAZINE LUIZA SA | codigo cvm: 22470\n",
      "Ano: 2023\n",
      "nome do arquivo: 22470_magazine_luiza_sa\n",
      "caminho dos arquivos: C:\\Users\\thgcn\\OneDrive\\Academico\\PO-245\\Projeto\\PO245\\22470_magazine_luiza_sa\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thgcn\\OneDrive\\Academico\\PO-245\\Projeto\\PO245\\po_245_2023_T4\\.venv\\Lib\\site-packages\\urllib3\\connectionpool.py:1095: InsecureRequestWarning: Unverified HTTPS request is being made to host 'www.rad.cvm.gov.br'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download finalizado. Arquivo salvo em: C:\\Users\\thgcn\\OneDrive\\Academico\\PO-245\\Projeto\\PO245\\22470_magazine_luiza_sa\\2023\\relatorio_de_agente_fiduciario\\2470_MAGAZINE LUIZA SA_0ª emissão Debênture_2023-04-28.pdf\n",
      "Inicio do pipeline de preprocessamento e indexacao *********\n",
      "Documento adicionado com sucesso!\n",
      "Documento adicionado com sucesso!\n",
      "Documento adicionado com sucesso!\n",
      "Documento adicionado com sucesso!\n",
      "Documento adicionado com sucesso!\n",
      "Documento adicionado com sucesso!\n",
      "Documento adicionado com sucesso!\n",
      "Documento adicionado com sucesso!\n",
      "Documento adicionado com sucesso!\n",
      "Documento adicionado com sucesso!\n",
      "Documento adicionado com sucesso!\n",
      "Documento adicionado com sucesso!\n"
     ]
    }
   ],
   "source": [
    "pipeline(\"magazine luiza\", 2023)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
